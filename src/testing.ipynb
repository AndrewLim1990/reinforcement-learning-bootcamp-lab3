{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import time\n",
    "import chainer as C\n",
    "import chainer.functions as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import click\n",
    "import gym\n",
    "\n",
    "from simpledqn.replay_buffer import ReplayBuffer\n",
    "import logger\n",
    "from simpledqn.wrappers import NoopResetEnv, EpisodicLifeEnv\n",
    "\n",
    "nprs = np.random.RandomState\n",
    "\n",
    "\n",
    "def assert_allclose(a, b):\n",
    "    if isinstance(a, (np.ndarray, float, int)):\n",
    "        np.testing.assert_allclose(a, b)\n",
    "    elif isinstance(a, (tuple, list)):\n",
    "        assert isinstance(b, (tuple, list))\n",
    "        assert len(a) == len(b)\n",
    "        for a_i, b_i in zip(a, b):\n",
    "            assert_allclose(a_i, b_i)\n",
    "    elif isinstance(a, C.Variable):\n",
    "        assert isinstance(b, C.Variable)\n",
    "        assert_allclose(a.data, b.data)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "rng = nprs(42)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "\n",
    "class Adam(object):\n",
    "    def __init__(self, shape, stepsize, beta1=0.9, beta2=0.999, epsilon=1e-08):\n",
    "        self.stepsize, self.beta1, self.beta2, self.epsilon = stepsize, beta1, beta2, epsilon\n",
    "        self.t = 0\n",
    "        self.v = np.zeros(shape, dtype=np.float32)\n",
    "        self.m = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "    def step(self, g):\n",
    "        self.t += 1\n",
    "        a = self.stepsize * \\\n",
    "            np.sqrt(1 - self.beta2 ** self.t) / (1 - self.beta1 ** self.t)\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (g * g)\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * g\n",
    "        step = - a * self.m / (np.sqrt(self.v) + self.epsilon)\n",
    "        return step\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "\n",
    "class NN(object):\n",
    "    \"\"\"Simple transparent neural network (multilayer perceptron) model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dims=None, out_fn=None):\n",
    "        assert dims is not None\n",
    "        assert out_fn is not None\n",
    "        assert len(dims) >= 2\n",
    "\n",
    "        self._out_fn = out_fn\n",
    "        self.lst_w, self.lst_b = [], []\n",
    "        for i in range(len(dims) - 1):\n",
    "            shp = dims[i + 1], dims[i]\n",
    "            # Correctly init weights.\n",
    "            std = 0.01 if i == len(dims) - 2 else 1.0\n",
    "            out = rng.randn(*shp).astype(np.float32)\n",
    "            out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "            self.lst_w.append(C.Variable(out))\n",
    "            self.lst_b.append(C.Variable(np.zeros(shp[0], dtype=np.float32)))\n",
    "        self.train_vars = self.lst_w + self.lst_b\n",
    "\n",
    "    def set_params(self, params):\n",
    "        lst_wt, lst_bt = params\n",
    "        for w, wt in zip(self.lst_w, lst_wt):\n",
    "            w.data[...] = wt.data\n",
    "        for b, bt in zip(self.lst_b, lst_bt):\n",
    "            b.data[...] = bt.data\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.lst_w, self.lst_b\n",
    "\n",
    "    def dump(self, file_path=None):\n",
    "        file = open(file_path, 'wb')\n",
    "        pickle.dump(dict(w=self.lst_w, b=self.lst_b), file, -1)\n",
    "        file.close()\n",
    "\n",
    "    def load(self, file_path=None):\n",
    "        file = open(file_path, 'rb')\n",
    "        params = pickle.load(file)\n",
    "        file.close()\n",
    "        return params['w'], params['b']\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (w, b) in enumerate(zip(self.lst_w, self.lst_b)):\n",
    "            x = F.linear(x, w, b)\n",
    "            if i != len(self.lst_w) - 1:\n",
    "                x = F.tanh(x)\n",
    "            else:\n",
    "                return self._out_fn(x)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "\n",
    "def preprocess_obs_gridworld(obs):\n",
    "    return obs.astype(np.float32)\n",
    "\n",
    "\n",
    "def preprocess_obs_ram(obs):\n",
    "    return obs.astype(np.float32) / 255.\n",
    "\n",
    "\n",
    "class LinearSchedule(object):\n",
    "    def __init__(self, schedule_timesteps, final_p, initial_p):\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.final_p = final_p\n",
    "        self.initial_p = initial_p\n",
    "\n",
    "    def value(self, t):\n",
    "        fraction = min(1.0, float(t) / self.schedule_timesteps)\n",
    "        return self.initial_p + (self.final_p - self.initial_p) * fraction\n",
    "\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self, env, get_obs_dim, get_act_dim, obs_preprocessor, replay_buffer, q_dim_hid,\n",
    "                 opt_batch_size, discount, initial_step, max_steps, learning_start_itr, target_q_update_freq,\n",
    "                 train_q_freq,\n",
    "                 log_freq, double_q, final_eps, initial_eps, fraction_eps, render):\n",
    "        self._env = env\n",
    "        self._get_obs_dim = get_obs_dim\n",
    "        self._get_act_dim = get_act_dim\n",
    "        self._obs_preprocessor = obs_preprocessor\n",
    "        self._replay_buffer = replay_buffer\n",
    "        self._initial_step = initial_step\n",
    "        self._max_steps = max_steps\n",
    "        self._target_q_update_freq = target_q_update_freq\n",
    "        self._learning_start_itr = learning_start_itr\n",
    "        self._train_q_freq = train_q_freq\n",
    "        self._log_freq = log_freq\n",
    "        self._double_q = double_q\n",
    "        self._act_dim = env.action_space.n\n",
    "        self._opt_batch_size = opt_batch_size\n",
    "        self._discount = discount\n",
    "        self._render = render\n",
    "        nn_args = dict(\n",
    "            dims=[self._get_obs_dim(env)] + q_dim_hid +\n",
    "            [self._get_act_dim(env)],\n",
    "            out_fn=lambda x: x)\n",
    "        # Q-function, Q(s,a,\\theta)\n",
    "        self._q = NN(**nn_args)\n",
    "        # Target Q-function, Q(s,a,\\theta')\n",
    "        self._qt = NN(**nn_args)\n",
    "        self.lst_adam = [Adam(var.shape, stepsize=1e-4)\n",
    "                         for var in self._q.train_vars]\n",
    "        self.exploration = LinearSchedule(\n",
    "            schedule_timesteps=int(fraction_eps * max_steps),\n",
    "            initial_p=initial_eps,\n",
    "            final_p=final_eps)\n",
    "\n",
    "    def eps_greedy(self, obs, epsilon):\n",
    "        # Check Q function, do argmax.\n",
    "        rnd = rng.rand()\n",
    "        if rnd > epsilon:\n",
    "            obs = self._obs_preprocessor(obs)\n",
    "            q_values = self._q.forward(obs)\n",
    "            return F.argmax(q_values, axis=1).data[0]\n",
    "        else:\n",
    "            return rng.randint(0, self._act_dim)\n",
    "\n",
    "\n",
    "    def compute_double_q_learning_loss(self, l_obs, l_act, l_rew, l_next_obs, l_done):\n",
    "        \"\"\"\n",
    "        :param l_obs: A chainer variable holding a list of observations. Should be of shape N * |S|.\n",
    "        :param l_act: A chainer variable holding a list of actions. Should be of shape N.\n",
    "        :param l_rew: A chainer variable holding a list of rewards. Should be of shape N.\n",
    "        :param l_next_obs: A chainer variable holding a list of observations at the next time step. Should be of\n",
    "        shape N * |S|.\n",
    "        :param l_done: A chainer variable holding a list of binary values (indicating whether episode ended after this\n",
    "        time step). Should be of shape N.\n",
    "        :return: A chainer variable holding a scalar loss.\n",
    "        \"\"\"\n",
    "        # Hint: You may want to make use of the following fields: self._discount, self._q, self._qt\n",
    "        # Hint2: Q-function can be called by self._q.forward(argument)\n",
    "        # Hint3: You might also find https://docs.chainer.org/en/stable/reference/generated/chainer.functions.select_item.html useful\n",
    "        loss = C.Variable(np.array([0.]))  # TODO: replace this line\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        return loss\n",
    "\n",
    "    def train_q(self, l_obs, l_act, l_rew, l_next_obs, l_done):\n",
    "        \"\"\"Update Q-value function by sampling from the replay buffer.\"\"\"\n",
    "\n",
    "        l_obs = self._obs_preprocessor(l_obs)\n",
    "        l_next_obs = self._obs_preprocessor(l_next_obs)\n",
    "        if self._double_q:\n",
    "            loss = self.compute_double_q_learning_loss(\n",
    "                l_obs, l_act, l_rew, l_next_obs, l_done)\n",
    "        else:\n",
    "            loss = self.compute_q_learning_loss(\n",
    "                l_obs, l_act, l_rew, l_next_obs, l_done)\n",
    "        for var in self._q.train_vars:\n",
    "            var.cleargrad()\n",
    "        loss.backward()\n",
    "        for var, adam in zip(self._q.train_vars, self.lst_adam):\n",
    "            var.data += adam.step(var.grad)\n",
    "        return loss.data\n",
    "\n",
    "    def _update_target_q(self):\n",
    "        \"\"\"Update the target Q-value function by copying the current Q-value function weights.\"\"\"\n",
    "        q_params = self._q.get_params()\n",
    "        self._qt.set_params(q_params)\n",
    "\n",
    "    def train(self):\n",
    "        obs = self._env.reset()\n",
    "\n",
    "        episode_rewards = []\n",
    "        n_episodes = 0\n",
    "        l_episode_return = deque([], maxlen=10)\n",
    "        l_discounted_episode_return = deque([], maxlen=10)\n",
    "        l_tq_squared_error = deque(maxlen=50)\n",
    "        log_itr = -1\n",
    "        for itr in range(self._initial_step, self._max_steps):\n",
    "            act = self.eps_greedy(obs[np.newaxis, :],\n",
    "                                  self.exploration.value(itr))\n",
    "            next_obs, rew, done, _ = self._env.step(act)\n",
    "            if self._render:\n",
    "                self._env.render()\n",
    "            self._replay_buffer.add(obs, act, rew, next_obs, float(done))\n",
    "\n",
    "            episode_rewards.append(rew)\n",
    "\n",
    "            if done:\n",
    "                obs = self._env.reset()\n",
    "                episode_return = np.sum(episode_rewards)\n",
    "                discounted_episode_return = np.sum(\n",
    "                    episode_rewards * self._discount ** np.arange(len(episode_rewards)))\n",
    "                l_episode_return.append(episode_return)\n",
    "                l_discounted_episode_return.append(discounted_episode_return)\n",
    "                episode_rewards = []\n",
    "                n_episodes += 1\n",
    "            else:\n",
    "                obs = next_obs\n",
    "\n",
    "            if itr % self._target_q_update_freq == 0 and itr > self._learning_start_itr:\n",
    "                self._update_target_q()\n",
    "\n",
    "            if itr % self._train_q_freq == 0 and itr > self._learning_start_itr:\n",
    "                # Sample from replay buffer.\n",
    "                l_obs, l_act, l_rew, l_obs_prime, l_done = self._replay_buffer.sample(\n",
    "                    self._opt_batch_size)\n",
    "                # Train Q value function with sampled data.\n",
    "                td_squared_error = self.train_q(\n",
    "                    l_obs, l_act, l_rew, l_obs_prime, l_done)\n",
    "                l_tq_squared_error.append(td_squared_error)\n",
    "\n",
    "            if (itr + 1) % self._log_freq == 0 and len(l_episode_return) > 5:\n",
    "                log_itr += 1\n",
    "                logger.logkv('Iteration', log_itr)\n",
    "                logger.logkv('Steps', itr)\n",
    "                logger.logkv('Epsilon', self.exploration.value(itr))\n",
    "                logger.logkv('Episodes', n_episodes)\n",
    "                logger.logkv('AverageReturn', np.mean(l_episode_return))\n",
    "                logger.logkv('AverageDiscountedReturn',\n",
    "                             np.mean(l_discounted_episode_return))\n",
    "                logger.logkv('TDError^2', np.mean(l_tq_squared_error))\n",
    "                logger.dumpkvs()\n",
    "                self._q.dump(logger.get_dir() + '/weights.pkl')\n",
    "\n",
    "    def test(self, epsilon):\n",
    "        try:\n",
    "            self._q.set_params(self._q.load(logger.get_dir() + '/weights.pkl'))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        obs = self._env.reset()\n",
    "        while True:\n",
    "            act = self.eps_greedy(obs[np.newaxis, :], epsilon)\n",
    "            obs_prime, rew, done, _ = self._env.step(act)\n",
    "            self._env.render()\n",
    "            if done:\n",
    "                obs = self._env.reset()\n",
    "                print('Done!')\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                obs = obs_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-08-06 21:50:18,123] Making new env: GridWorld-v0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DQN' object has no attribute 'compute_q_learning_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-326dc7a14445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.909377098083496\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mactual_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_q_learning_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mtest_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"compute_q_learning_loss\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         assert isinstance(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DQN' object has no attribute 'compute_q_learning_loss'"
     ]
    }
   ],
   "source": [
    "env_id = 'GridWorld-v0'\n",
    "double = False\n",
    "render = False\n",
    "\n",
    "if env_id == 'GridWorld-v0':\n",
    "    from simpledqn import gridworld_env\n",
    "    env = gym.make('GridWorld-v0')\n",
    "\n",
    "    def get_obs_dim(x): return x.observation_space.n\n",
    "\n",
    "    def get_act_dim(x): return x.action_space.n\n",
    "    obs_preprocessor = preprocess_obs_gridworld\n",
    "    max_steps = 100000\n",
    "    log_freq = 1000\n",
    "    target_q_update_freq = 100\n",
    "    initial_step = 0\n",
    "    log_dir = \"data/local/dqn_gridworld\"\n",
    "\n",
    "logger.session(log_dir).__enter__()\n",
    "env.seed(42)\n",
    "\n",
    "# Initialize the replay buffer that we will use.\n",
    "replay_buffer = ReplayBuffer(max_size=10000)\n",
    "\n",
    "# Initialize DQN training procedure.\n",
    "dqn = DQN(\n",
    "    env=env,\n",
    "    get_obs_dim=get_obs_dim,\n",
    "    get_act_dim=get_act_dim,\n",
    "    obs_preprocessor=obs_preprocessor,\n",
    "    replay_buffer=replay_buffer,\n",
    "\n",
    "    # Q-value parameters\n",
    "    q_dim_hid=[256, 256] if env_id == 'Pong-ram-v0' else [],\n",
    "    opt_batch_size=64,\n",
    "\n",
    "    # DQN gamma parameter\n",
    "    discount=0.99,\n",
    "\n",
    "    # Training procedure length\n",
    "    initial_step=initial_step,\n",
    "    max_steps=max_steps,\n",
    "    learning_start_itr=max_steps // 100,\n",
    "    # Frequency of copying the actual Q to the target Q\n",
    "    target_q_update_freq=target_q_update_freq,\n",
    "    # Frequency of updating the Q-value function\n",
    "    train_q_freq=4,\n",
    "    # Double Q\n",
    "    double_q=double,\n",
    "\n",
    "    # Exploration parameters\n",
    "    initial_eps=1.0,\n",
    "    final_eps=0.05,\n",
    "    fraction_eps=0.1,\n",
    "\n",
    "    # Logging\n",
    "    log_freq=log_freq,\n",
    "    render=render,\n",
    ")\n",
    "\n",
    "if env_id == 'GridWorld-v0':\n",
    "    # Run tests on GridWorld-v0\n",
    "    test_args = dict(\n",
    "        l_obs=nprs(0).rand(64, 16).astype(np.float32),\n",
    "        l_act=nprs(1).randint(0, 3, size=(64,)),\n",
    "        l_rew=nprs(2).randint(0, 3, size=(64,)).astype(np.float32),\n",
    "        l_next_obs=nprs(3).rand(64, 16).astype(np.float32),\n",
    "        l_done=nprs(4).randint(0, 2, size=(64,)).astype(np.float32),\n",
    "    )\n",
    "    if not double:\n",
    "        tgt = np.array([1.909377098083496], dtype=np.float32)\n",
    "        actual_var = dqn.compute_q_learning_loss(**test_args)\n",
    "        test_name = \"compute_q_learning_loss\"\n",
    "        assert isinstance(\n",
    "            actual_var, C.Variable), \"%s should return a Chainer variable\" % test_name\n",
    "        actual = actual_var.data\n",
    "        try:\n",
    "            assert_allclose(tgt, actual)\n",
    "            print(\"Test for %s passed!\" % test_name)\n",
    "        except AssertionError as e:\n",
    "            pass\n",
    "#             print(\"Warning: test for %s didn't pass!\" % test_name)\n",
    "#             print(e)\n",
    "#             input(\n",
    "#                 \"** Test failed. Press Ctrl+C to exit or press enter to continue training anyways\")\n",
    "    else:\n",
    "        tgt = np.array([1.9066928625106812], dtype=np.float32)\n",
    "        actual_var = dqn.compute_double_q_learning_loss(**test_args)\n",
    "        test_name = \"compute_double_q_learning_loss\"\n",
    "        assert isinstance(\n",
    "            actual_var, C.Variable), \"%s should return a Chainer variable\" % test_name\n",
    "        actual = actual_var.data\n",
    "        try:\n",
    "            assert_allclose(tgt, actual)\n",
    "            print(\"Test for %s passed!\" % test_name)\n",
    "        except AssertionError as e:\n",
    "            print(\"Warning: test for %s didn't pass!\" % test_name)\n",
    "            print(e)\n",
    "            input(\n",
    "                \"** Test failed. Press Ctrl+C to exit or press enter to continue training anyways\")\n",
    "\n",
    "if render:\n",
    "    dqn.test(epsilon=0.0)\n",
    "else:\n",
    "    # Train the agent!\n",
    "    dqn.train()\n",
    "\n",
    "# Close gym environment.\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearSchedule(object):\n",
    "    def __init__(self, schedule_timesteps, final_p, initial_p):\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.final_p = final_p\n",
    "        self.initial_p = initial_p\n",
    "\n",
    "    def value(self, t):\n",
    "        fraction = min(1.0, float(t) / self.schedule_timesteps)\n",
    "        return self.initial_p + (self.final_p - self.initial_p) * fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99981"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction_eps = 0.1\n",
    "initial_eps = 1.0\n",
    "final_eps = 0.05\n",
    "\n",
    "exploration = LinearSchedule(\n",
    "    schedule_timesteps=int(fraction_eps * max_steps),\n",
    "    initial_p=initial_eps,\n",
    "    final_p=final_eps\n",
    ")\n",
    "\n",
    "exploration.value(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
